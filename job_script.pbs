#!/bin/bash

### Set the job name. Your output files will share this name.
#PBS -N BE_SHO
### Enter your email address. Errors will be emailed to this address.
#PBS -M somnaths@ornl.gov
### Node spec, number of nodes and processors per node that you desire.
### One node and 36 cores per node in this case.
#PBS -l nodes=1:ppn=36
### Tell PBS the anticipated runtime for your job, where walltime=HH:MM:S.
#PBS -l walltime=0:00:20:0
### The LDAP group list they need; cades-birthright in this case.
#PBS -W group_list=cades-ccsd
### Your account type. Birthright in this case.
#PBS -A ccsd
### Quality of service set to burst.
#PBS -l qos=std


## main program ##

### Remove old modules to ensure a clean state.
module purge

### Load modules (your programming environment)
# Normally would use PE-gnu but Ketan has compiled numpy to use MKL on Condo
module load PE-intel
### Load custom python virtual environment
module load python/3.6.3

### Check loaded modules
module list

### Forcing MKL to use 1 thread only:
# MUST KEEP THESE LINES. WILL NOT WORK OTHERWISE
export MKL_NUM_THREADS=1
export OPENBLAS_NUM_THREADS=1

EGNAME=be_sho
DATA_PATH=$HOME/BE_SHO_Fitting/data/BELine_0009_just_translation.h5
SCRIPTS_PATH=$HOME/BE_SHO_Fitting/code
WORK_PATH=/lustre/or-hydra/cades-ccsd/syz/pycroscopy_ensemble

cd $WORK_PATH
mkdir $EGNAME
cd $EGNAME

### Show current directory.
pwd

### Copy data:
DATA_NAME=be_line.h5
rm -rf $DATA_NAME
cp $DATA_PATH $DATA_NAME

### Copy python source code:
cp $SCRIPTS_PATH/*.py .

# See the contents in Lustre to make sure they are OK
ls -hl

### MPI run followed by the name/path of the binary.
mpiexec -use-hwthread-cpus python master.py $DATA_NAME
### mpiexec -use-hwthread-cpus python -m cProfile -s cumtime master.py $DATA_NAME
